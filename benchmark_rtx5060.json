{
  "benchmark_type": "throughput",
  "config": {
    "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "input_length": 512,
    "output_length": 128,
    "batch_size": 1,
    "num_prompts": 50,
    "dtype": "float16",
    "tensor_parallel_size": 1,
    "max_model_len": null,
    "gpu_memory_utilization": 0.9,
    "enable_prefix_caching": false
  },
  "success": true,
  "error_message": null,
  "metrics": {
    "mean_latency_ms": null,
    "p50_latency_ms": null,
    "p95_latency_ms": null,
    "p99_latency_ms": null,
    "throughput_tokens_per_sec": 0.4,
    "requests_per_sec": null,
    "time_to_first_token_ms": null,
    "time_per_output_token_ms": null,
    "inter_token_latency_ms": null,
    "peak_gpu_memory_gb": null
  }
}