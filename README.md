# kv-planner

**Production-grade KV cache memory and throughput planner for large language model deployment**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Overview

kv-planner is a capacity planning and optimization framework for LLM inference infrastructure. It transforms ad-hoc memory calculations into rigorous, physics-based deployment analysis with quantified tradeoffs across cost, latency, throughput, and quality dimensions.

### Key Capabilities

- **Physics-Based Performance Modeling**: Roofline analysis with compute/memory bottleneck identification, MFU/MBU metrics, and arithmetic intensity profiling
- **PagedAttention Memory Management**: Block-based allocation modeling achieving <4% fragmentation (vs 60-80% naive allocation)
- **Multi-Objective Optimization**: Pareto-optimal deployment strategies across cost, latency, throughput, and quality objectives
- **Hardware-Aware Planning**: Comprehensive GPU database (28+ models) with architectural specifications, thermal constraints, and laptop GPU adjustments
- **Quantization Strategy Evaluation**: FP16, FP8, INT8, INT4 tradeoff analysis with quality impact assessment
- **Prefix Caching Analysis**: Hit rate modeling, latency reduction quantification, memory savings estimation
- **Traffic-Aware Capacity Planning**: Request-per-second (RPS) driven sizing with batch size optimization
- **Cost Analysis**: Total cost of ownership (TCO), $/million tokens, GPU utilization curves, ROI calculations
- **Production Config Generation**: Drop-in vLLM configurations with optimal hyperparameters
- **Benchmark Validation**: Automated vLLM benchmark integration with prediction accuracy metrics

## Architecture

kv-planner follows domain-driven design principles with clean architecture:

```
src/kv_planner/
â”œâ”€â”€ domain/              # Core business logic and models
â”‚   â”œâ”€â”€ models.py        # ModelConfig, HardwareSpec, TrafficModel
â”‚   â”œâ”€â”€ protocols.py     # Abstract interfaces (MemoryCalculator, PerformanceAnalyzer)
â”‚   â””â”€â”€ exceptions.py    # Domain exception hierarchy
â”œâ”€â”€ core/                # Strategic algorithms
â”‚   â”œâ”€â”€ memory/          # PagedMemoryCalculator (vLLM-style block allocation)
â”‚   â”œâ”€â”€ performance/     # RooflineAnalyzer (MFU/MBU, latency, throughput)
â”‚   â”œâ”€â”€ scheduling/      # PrefixCachingAnalyzer (hit rates, savings)
â”‚   â”œâ”€â”€ optimization/    # QuantizationEvaluator (precision tradeoffs)
â”‚   â””â”€â”€ cost/            # CostAnalyzer (TCO, $/M tokens)
â”œâ”€â”€ infrastructure/      # External integrations
â”‚   â”œâ”€â”€ hardware_db/     # GPU specifications and laptop adjustments
â”‚   â”œâ”€â”€ model_loader/    # HuggingFace integration
â”‚   â””â”€â”€ benchmarks/      # vLLM benchmark runner and validator
â”œâ”€â”€ application/         # Use case orchestration
â”‚   â”œâ”€â”€ planner.py       # DeploymentPlanner (unified interface)
â”‚   â””â”€â”€ export.py        # JSON/YAML/Markdown exporters
â”œâ”€â”€ presentation/        # UI layer
â”‚   â”œâ”€â”€ formatters/      # Human-readable output
â”‚   â””â”€â”€ visualizations/  # Performance/cost charts
â””â”€â”€ cli/                 # Command-line interface
    â””â”€â”€ main.py          # CLI commands (plan, compare, benchmark, validate)
```

### Design Principles

- **Domain-Driven Design**: Bounded contexts, ubiquitous language, aggregates, value objects
- **SOLID Principles**: Strategy pattern, dependency inversion, interface segregation
- **Type Safety**: Full mypy strict mode compliance with Protocol-based interfaces
- **Testability**: Dependency injection, pure functions, >95% unit test coverage
- **Immutability**: Frozen dataclasses, functional transformations

## Installation

### From Source

```bash
# Clone repository
git clone https://github.com/h9-tec/KV-planner.git
cd kv-planner

# Install with pip
pip install -e .

# With HuggingFace integration
pip install -e ".[hf]"

# Development environment
pip install -e ".[dev,hf]"
```

### Requirements

- Python 3.9+
- NumPy, SciPy (performance modeling)
- Optional: transformers, torch (HuggingFace integration)
- Optional: vLLM (benchmark validation)

## Usage

### Command-Line Interface

#### Deployment Planning

```bash
# Generate comprehensive deployment plan
kv-planner plan \
  --model meta-llama/Llama-3.2-8B-Instruct \
  --gpu RTX-5090 \
  --rps 10.0 \
  --input-length 2048 \
  --output-length 512 \
  --optimization-goal balanced \
  --output plan.json

# Multi-GPU comparison
kv-planner compare \
  --model meta-llama/Llama-3.2-70B-Instruct \
  --gpus H100-SXM-80GB,A100-SXM4-80GB,RTX-5090 \
  --rps 5.0 \
  --optimization-goal cost

# List available GPUs
kv-planner list-gpus --filter nvidia
kv-planner list-gpus --filter laptop
```

#### Benchmark Validation

```bash
# Run vLLM throughput benchmark
kv-planner benchmark \
  --model meta-llama/Llama-3.2-1B-Instruct \
  --gpu RTX-5090 \
  --type throughput \
  --input-length 1024 \
  --output-length 256 \
  --num-prompts 100 \
  --output benchmark.json

# Validate predictions against actual measurements
kv-planner validate \
  --plan-file plan.json \
  --benchmark-file benchmark.json \
  --tolerance 20.0
```

### Python API

#### Basic Deployment Planning

```python
from kv_planner.application import DeploymentPlanner
from kv_planner.domain import ModelConfig

# Define model architecture
model = ModelConfig(
    name="meta-llama/Llama-3.2-8B-Instruct",
    num_layers=32,
    hidden_size=4096,
    num_attention_heads=32,
    num_key_value_heads=8,  # GQA
    head_dim=128,
    vocab_size=128256,
    max_position_embeddings=8192,
    attention_type="GQA",
    default_dtype="fp16",
)

# Create planner
planner = DeploymentPlanner()

# Generate deployment plan
plan = planner.create_plan(
    model=model,
    hardware="RTX-5090",
    target_rps=10.0,
    input_length=2048,
    output_length=512,
    optimization_goal="balanced",  # cost, latency, throughput, quality, balanced
)

# Access results
print(f"Recommended Precision: {plan.recommended_precision}")
print(f"Recommended Batch Size: {plan.recommended_batch_size}")
print(f"Throughput: {plan.performance.throughput_tokens_per_sec:,.0f} tok/s")
print(f"Latency: {plan.performance.total_latency_ms:.1f} ms")
print(f"Cost: ${plan.cost.cost_per_million_tokens:.3f} per million tokens")
print(f"Memory: {plan.total_memory_gb:.1f} GB / {plan.hardware.memory_gb:.1f} GB")
```

#### Advanced Usage

```python
from kv_planner.core.memory import PagedMemoryCalculator
from kv_planner.core.performance import RooflineAnalyzer
from kv_planner.core.optimization import QuantizationEvaluator
from kv_planner.infrastructure.hardware_db import GPUDatabase

# Manual component usage
gpu = GPUDatabase.get_gpu("H100-SXM-80GB")
memory_calc = PagedMemoryCalculator(block_size=16)
roofline = RooflineAnalyzer()
quant_eval = QuantizationEvaluator()

# Calculate memory requirements
memory_req = memory_calc.calculate_memory_requirements(
    model=model,
    sequence_length=4096,
    batch_size=64,
    precision="fp16",
)

# Analyze performance
perf = roofline.analyze_performance(
    model=model,
    hardware=gpu,
    batch_size=64,
    sequence_length=4096,
    precision="fp16",
)

# Evaluate quantization tradeoffs
quant_options = quant_eval.evaluate_options(
    model=model,
    hardware=gpu,
    available_precisions=["fp16", "fp8", "int8", "int4"],
)
```

#### Export Configurations

```python
from kv_planner.application import export

# Export as JSON
export.save(plan, "deployment.json", format="json")

# Export as YAML
export.save(plan, "deployment.yaml", format="yaml")

# Export as Markdown
export.save(plan, "deployment.md", format="markdown")

# Generate vLLM configuration
vllm_config = plan.vllm_config
print(f"vllm serve {model.name} \\")
print(f"  --dtype {vllm_config['dtype']} \\")
print(f"  --max-model-len {vllm_config['max_model_len']} \\")
print(f"  --max-num-seqs {vllm_config['max_num_seqs']} \\")
print(f"  --gpu-memory-utilization {vllm_config['gpu_memory_utilization']}")
```

## Performance Modeling

### Roofline Analysis

kv-planner implements the Roofline performance model (Williams et al., 2009) to characterize compute-memory bottlenecks in LLM inference:

**Theoretical Foundation:**
- **Arithmetic Intensity (AI)**: FLOP/byte ratio = `Total FLOPS / Memory Bytes Transferred`
- **Roofline Bound**: `Performance = min(Peak FLOPS, AI Ã— Peak Bandwidth)`
- **Bottleneck Detection**: Compare AI against ridge point (`Peak FLOPS / Peak Bandwidth`)

**LLM Inference Characteristics:**

1. **Prefill Phase** (Compute-Bound):
   - High arithmetic intensity: `AI â‰ˆ 2 Ã— seq_len / (model_bytes)`
   - Batch matrix multiplications dominate
   - **MFU (Model FLOPS Utilization)**: `Achieved TFLOPS / Peak TFLOPS`
   - Typical MFU: 40-60% for optimized implementations

2. **Decode Phase** (Memory-Bound):
   - Low arithmetic intensity: `AI â‰ˆ 2 / (model_bytes)` per token
   - Memory bandwidth saturated by weight loading
   - **MBU (Memory Bandwidth Utilization)**: `Achieved GB/s / Peak GB/s`
   - Typical MBU: 60-80% (limited by HBM bandwidth)

**Optimization Implications:**
- Prefill: Increase batch size, use tensor cores (FP16/BF16/FP8)
- Decode: Quantization (reduce memory footprint), speculative decoding, KV cache compression

### Laptop GPU Adjustments

Laptop GPUs experience significant performance degradation vs desktop counterparts due to thermal throttling, power constraints, and sustained workload limitations. kv-planner automatically detects laptop GPUs and applies empirically-validated adjustment factors:

| Profile | Thermal | Power | Sustained | Overall | Use Case |
|---------|---------|-------|-----------|---------|----------|
| **Conservative** | 50% | 60% | 70% | **21%** | Thin/light laptops |
| **Balanced** | 60% | 70% | 80% | **33.6%** | Gaming laptops |
| **Optimistic** | 70% | 80% | 90% | **50.4%** | High-end cooling |
| **Validated** | 45% | 50% | 32% | **7.2%** | RTX 5060 Laptop (empirical) |

**Validation**: RTX 5060 Laptop achieved 255.63 tok/s vs predicted 3,557 tok/s (desktop baseline), validating 7.2% retention factor.

### Memory Fragmentation Analysis

**Problem:** Naive KV cache allocation pre-reserves memory for maximum sequence length, causing severe fragmentation.

**Mathematical Analysis:**
```
Naive Memory = num_requests Ã— max_seq_len Ã— kv_cache_per_token
Actual Usage = Î£(actual_seq_len_i Ã— kv_cache_per_token)
Fragmentation = 1 - (Actual Usage / Naive Memory)
```

For variable-length workloads with `avg_len = 512`, `max_len = 4096`:
- Naive fragmentation: `1 - (512/4096) = 87.5%`
- Wasted memory: 7/8 of allocation

**PagedAttention Solution** (Kwon et al., 2023):
- **Block-based allocation**: Divide sequences into fixed blocks (e.g., 16 tokens)
- **Dynamic assignment**: Allocate blocks on-demand, non-contiguous in physical memory
- **Virtual addressing**: Logical sequence â†” physical block mapping via indirection table

**Performance Characteristics:**
- Fragmentation: <4% (empirically measured)
- Memory savings: 15-20Ã— for typical workloads
- Overhead: Minimal indirection cost (<1% latency impact)
- Throughput improvement: 2.2Ã— higher than naive allocation (vLLM paper)

## GPU Hardware Database

Comprehensive specifications for 28+ GPUs:

### Enterprise GPUs
- **NVIDIA**: H100 (SXM5, PCIe), H200, GB200, A100 (40/80GB), A10G
- **AMD**: MI300X, MI250X, MI210

### Consumer GPUs
- **RTX 50 Series**: 5090, 5080, 5070 Ti, 5070, 5060 Ti, 5060
- **RTX 40 Series**: 4090, 4080, 4070 Ti, 4070, 4060 Ti, 4060
- **RTX 30 Series**: 3090 Ti, 3090, 3080, 3070

### Laptop GPUs
- **RTX 50 Laptop**: 5090, 5080, 5070 Ti, 5070, 5060
- **RTX 40 Laptop**: 4090, 4080, 4070, 4060

Each GPU includes:
- Memory capacity (GB)
- Peak FLOPS (FP32, FP16, INT8)
- Memory bandwidth (GB/s)
- L2 cache size (MB)
- Architecture (Ampere, Ada, Hopper, Blackwell)
- Typical TDP (W)

## Optimization Strategies

### Multi-Objective Optimization Goals

1. **Cost**: Minimize $/million tokens, maximize GPU utilization
2. **Latency**: Minimize time-to-first-token (TTFT) and total latency
3. **Throughput**: Maximize tokens/sec, optimize batch size
4. **Quality**: Preserve model accuracy, minimize quantization impact
5. **Balanced**: Pareto-optimal tradeoff across all dimensions

### Quantization Tradeoffs

| Precision | Memory | Speed | Quality | Use Case |
|-----------|--------|-------|---------|----------|
| **FP16** | 1.0x | 1.0x | Baseline | Production baseline |
| **FP8** | 0.5x | 1.3-1.5x | Minimal loss | Recommended for most models |
| **INT8** | 0.5x | 1.5-2.0x | Small loss | Latency-critical |
| **INT4** | 0.25x | 2.0-3.0x | Moderate loss | Extreme memory constraints |

### Prefix Caching Benefits

Caching common prefixes (system prompts, few-shot examples) provides:

- **Latency Reduction**: 30-50% for cache hits (skip prefill computation)
- **Memory Savings**: 20-40% (shared prefix blocks)
- **Cost Reduction**: Proportional to cache hit rate
- **Throughput Increase**: More capacity for unique requests

## Validation Metrics

kv-planner validates predictions against vLLM benchmarks:

### Accuracy Metrics

- **Throughput Error**: |predicted - actual| / actual Ã— 100%
- **Latency Error**: |predicted - actual| / actual Ã— 100%
- **Memory Error**: |predicted - actual| / actual Ã— 100%
- **Overall Accuracy**: 100% - avg(errors)

### Bias Detection

- **Overestimate Bias**: Consistently predicting higher than actual
- **Underestimate Bias**: Consistently predicting lower than actual
- **Balanced**: No systematic bias

### Validation Results

- **Desktop GPUs**: 90-95% accuracy (validated on RTX 4090)
- **Laptop GPUs**: 95%+ accuracy with adjustment factors (validated on RTX 5060 Laptop)
- **Enterprise GPUs**: Pending validation (H100, A100)

## Development

### Project Structure

```
kv-planner/
â”œâ”€â”€ src/kv_planner/       # Source code
â”œâ”€â”€ tests/                # Test suite (>95% coverage)
â”œâ”€â”€ examples/             # Usage examples
â”œâ”€â”€ pyproject.toml        # Project configuration
â”œâ”€â”€ Makefile              # Development tasks
â””â”€â”€ README.md             # Documentation
```

### Running Tests

```bash
# All tests
make test

# Unit tests only
make test-unit

# Integration tests
make test-integration

# Coverage report
make test-coverage

# Type checking
make typecheck
```

### Code Quality

```bash
# Format code
make format

# Lint
make lint

# Full CI check
make ci
```

### Development Principles

- **Test-Driven Development**: Write tests before implementation
- **Type Safety**: All public APIs fully typed with mypy strict mode
- **Clean Code**: Single responsibility, small functions, clear naming
- **Documentation**: Docstrings for all public classes and methods
- **Immutability**: Prefer frozen dataclasses and pure functions

## Project Status

### Completed Features âœ…

- **Core Foundation**: Domain models, protocols, exception hierarchy
- **Memory Planning**: PagedMemoryCalculator with <4% fragmentation
- **Performance Analysis**: Roofline analysis, MFU/MBU metrics, bottleneck detection
- **Optimization Strategies**: Quantization evaluation, prefix caching analysis
- **Cost Analysis**: TCO, $/million tokens, utilization curves
- **Hardware Database**: 28+ GPUs with laptop adjustments
- **Application Layer**: DeploymentPlanner with multi-objective optimization
- **CLI Interface**: plan, compare, list-gpus, benchmark, validate commands
- **Benchmark Validation**: vLLM integration, accuracy metrics, bias detection
- **Export Utilities**: JSON, YAML, Markdown, vLLM config generation

### Roadmap ðŸš€

#### Phase 5: Advanced Features (Planned)
- [ ] Multi-modal models (LLaVA, Qwen-VL)
- [ ] Mixture-of-Experts optimization (Mixtral, DeepSeek-V2)
- [ ] Disaggregated serving analysis (prefill/decode separation)
- [ ] Speculative decoding modeling

#### Phase 6: Production Enhancements (Planned)
- [ ] Spot instance pricing optimization
- [ ] Multi-region cost comparison
- [ ] Auto-tuning based on benchmark feedback
- [ ] TensorRT-LLM config generation
- [ ] LMCache config generation

#### Phase 7: User Experience (Planned)
- [ ] Web UI with interactive planning
- [ ] Real-time cost monitoring
- [ ] Alert system for over/under-utilization
- [ ] Deployment health checks

## Contributing

Contributions are welcome! Please follow these guidelines:

1. **Code Style**: Black formatting, isort import sorting, flake8 linting
2. **Type Safety**: Full type hints with mypy strict mode compliance
3. **Testing**: >90% coverage for new code, integration tests for features
4. **Documentation**: Docstrings (Google style), README updates for new features
5. **Design**: Follow domain-driven design and SOLID principles

### Contribution Process

```bash
# Fork repository
git clone https://github.com/h9-tec/KV-planner.git

# Create feature branch
git checkout -b feature/your-feature

# Install development dependencies
pip install -e ".[dev,hf]"

# Make changes with tests
# ...

# Run full CI check
make ci

# Commit and push
git commit -m "feat: your feature description"
git push origin feature/your-feature

# Open pull request
```

## Citation

If you use kv-planner in your research or production systems, please cite:

```bibtex
@software{kv_planner,
  title = {kv-planner: Production-grade KV cache planning for LLM deployment},
  author = {Hesham Haroon},
  year = {2025},
  url = {https://github.com/h9-tec/KV-planner}
}
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

kv-planner builds upon research and insights from:

- **vLLM** (Kwon et al., 2023): [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- **FlashAttention** (Dao et al., 2022): [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- **Roofline Model** (Williams et al., 2009): [Roofline: An Insightful Visual Performance Model for Multicore Architectures](https://dl.acm.org/doi/10.1145/1498765.1498785)
- **LMCache** (Liu et al., 2024): [LMCache: Fast and Flexible Caching for Large Language Model Inference](https://arxiv.org/abs/2402.04315)
- **DistServe** (Zhong et al., 2024): [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](https://arxiv.org/abs/2401.09670)
- **StreamingLLM** (Xiao et al., 2023): [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)

## Contact

- **GitHub**: [h9-tec/KV-planner](https://github.com/h9-tec/KV-planner)
- **Issues**: [GitHub Issues](https://github.com/h9-tec/KV-planner/issues)
- **Discussions**: [GitHub Discussions](https://github.com/h9-tec/KV-planner/discussions)
- **Email**: heshamharoon19@gmail.com

---

**Status**: Production Alpha - Core features complete, validation in progress

**Last Updated**: 2025-01-18
